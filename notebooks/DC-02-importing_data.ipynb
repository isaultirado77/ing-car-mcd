{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bdeb6f",
   "metadata": {},
   "source": [
    "# **02 Intermediate importing data in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb79c66",
   "metadata": {},
   "source": [
    "## 1 Importing flat files from the *web*\n",
    "\n",
    "* Important packages: `urllib`, `requests`\n",
    "\n",
    "* Using `urllib`: \n",
    "```python\n",
    "    from urllib.request import urlretrive\n",
    "    url = \"https.../data.csv\"\n",
    "    urlretrive(url, 'data_location')\n",
    "```  \n",
    "* Get `html` data: \n",
    "```python\n",
    "    from urllib.request import urlopen, Request\n",
    "    url = \"https://www.page.org\"\n",
    "    request = Request(url)\n",
    "    response = urlopen(request)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c724bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8b0bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" class=\"no-js\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<title>Wikipedia</title>\n",
      "<meta name=\"description\" content=\"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\">\n",
      "<script>\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)no-js(\\s|$)/, \"$1js-enabled$2\" );\n",
      "</script>\n",
      "<meta name=\"viewport\" content=\"initial-scale=1,user-scalable=yes\">\n",
      "<link rel=\"apple-touch-\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.wikipedia.org/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "request = Request(url, headers=headers)\n",
    "response = urlopen(request)\n",
    "html = response.read().decode(\"utf-8\")\n",
    "response.close()\n",
    "\n",
    "print(html[:500]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367b10a",
   "metadata": {},
   "source": [
    "* using `requests `package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af276495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\" class=\"no-js\">\\n<head>\\n<meta charset=\"utf-8\">\\n<title>Wikipedia</title>\\n<meta name=\"description\" content=\"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\">\\n<script>\\ndocument.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)no-js(\\\\s|$)/, \"$1js-enabled$2\" );\\n</script>\\n<meta name=\"viewport\" content=\"initial-scale=1,user-scalable=yes\">\\n<link rel=\"apple-touch-'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "r = requests.get(url, headers=headers)\n",
    "text = r.text\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c12ed6",
   "metadata": {},
   "source": [
    "* using `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeab4e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5579c31",
   "metadata": {},
   "source": [
    "### Scrapping with BeautifulSoup\n",
    "\n",
    "* BSoup allos parse and stract data from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51603789",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.crummy.com/software/BeautifulSoup/\"\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cfc53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bb3e2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<title>Beautiful Soup: We called him Tortoise because he taught us.</title>,\n",
       " bs4.element.Tag)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title, type(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "431e1ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nBeautiful Soup: We called him Tortoise because he taught us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[ Download | Documentation | Hall of Fame | For enterprise | Source | Changelog | Discussion group  | Zine ]\\n\\nBeautiful Soup\\n\\nYou didn\\'t write that awful page. You\\'re just trying to get some\\ndata out of it. Beautiful Soup is here to help. Since 2004, it\\'s been\\nsaving programmers hours or days of work on quick-turnaround\\nscreen scraping projects.\\nBeautiful Soup is a Python library designed for quick turnaround\\nprojects like screen-scraping. Three features make it powerful:\\n\\n\\nBeautiful Soup provides a few simple methods and Pythonic idioms\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\ndissecting a document and extracting what you need. It doesn\\'t take\\nmuch code to write an application\\n\\nBeautiful Soup automatically converts incoming documents to\\nUnicode and outgoing documents to UTF-8. You don\\'t have to think\\nabout encodings, unless the document doesn\\'t specify an encoding and\\nBeautiful Soup can\\'t detect one. Then you just have to specify the\\noriginal encoding.\\n\\nBeautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you\\nto try out different parsing strategies or trade speed for\\nflexibility.\\n\\n\\nBeautiful Soup parses anything you give it, and does the tree\\ntraversal stuff for you. You can tell it \"Find all the links\", or\\n\"Find all the links of class externalLink\", or \"Find all the\\nlinks whose urls match \"foo.com\", or \"Find the table heading that\\'s\\ngot bold text, then give me that text.\"\\n\\nValuable data that was once locked up in poorly-designed websites\\nis now within your reach. Projects that would have taken hours take\\nonly minutes with Beautiful Soup.\\n\\nInterested? Read more.\\nGetting and giving support\\n\\n\\n\\n  Beautiful Soup for enterprise available via Tidelift\\n \\n\\n\\nIf you have questions, send them to the discussion\\ngroup. If you find a bug, file it on Launchpad. If it\\'s a security vulnerability, report it confidentially through Tidelift.\\nIf you use Beautiful Soup as part of your work, please consider a Tidelift subscription. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n\\n\\nIf Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\\nDownload Beautiful Soup\\nThe current release is Beautiful Soup\\n4.13.5 (August 24, 2025). You can install Beautiful Soup 4 with\\npip install beautifulsoup4.\\n\\nIn Debian and Ubuntu, Beautiful Soup is available as the\\npython3-bs4 package. In Fedora it\\'s\\navailable as the python3-beautifulsoup4 package.\\n\\nBeautiful Soup is licensed under the MIT license, so you can also\\ndownload the tarball, drop the bs4/ directory into almost\\nany Python application (or into your library path) and start using it\\nimmediately.\\n\\nBeautiful Soup 4 is supported on Python versions 3.7 and\\ngreater. Support for Python 2 was discontinued on January 1, 2021—one\\nyear after the Python 2 sunsetting date.\\n\\nBeautiful Soup 3\\nBeautiful Soup 3 was the official release line of Beautiful Soup\\nfrom May 2006 to March 2012. It does not support Python 3 and was\\ndiscontinued or January 1, 2021—one year after the Python 2\\nsunsetting date. If you have any active projects using Beautiful Soup\\n3, you should migrate to Beautiful Soup 4 as part of your Python 3\\nconversion.\\n\\nSoon, the beautifulsoup PyPI package name will be reclaimed by a more recent version of Beautiful Soup. When that happens,\\nall Python 2 code still using Beautiful Soup 3 will break.\\n\\nHere\\'s\\nthe Beautiful Soup 3 documentation.\\nThe current and hopefully final release of Beautiful Soup 3 is 3.2.2 (October 5,\\n2019). It\\'s the BeautifulSoup package on pip. It\\'s also\\navailable as python-beautifulsoup in Debian and Ubuntu,\\nand as python-BeautifulSoup in Fedora.\\n\\nBeautiful Soup 3, like Beautiful Soup 4, is supported through Tidelift.\\nHall of Fame\\nOver the years, Beautiful Soup has been used in hundreds of\\ndifferent projects. There\\'s no way I can list them all, but I want to\\nhighlight a few high-profile projects. Beautiful Soup isn\\'t what makes\\nthese projects interesting, but it did make their completion easier:\\n\\n\\n\"Movable\\n Type\", a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\nJiabao Lin\\'s DXY-COVID-19-Crawler\\nuses Beautiful Soup to scrape a Chinese medical site for information\\nabout COVID-19, making it easier for researchers to track the spread\\nof the virus. (Source: \"How open source software is fighting COVID-19\")\\n\\nReddit uses Beautiful Soup to parse\\na page that\\'s been linked to and find a representative image.\\n\\nAlexander Harrowell uses Beautiful Soup to track the business\\n activities of an arms merchant.\\n\\nThe developers of Python itself used Beautiful Soup to migrate the Python\\nbug tracker from Sourceforge to Roundup.\\n\\nThe Lawrence Journal-World\\nuses Beautiful Soup to gather\\nstatewide election results.\\n\\nThe NOAA\\'s Forecast\\nApplications Branch uses Beautiful Soup in TopoGrabber, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n\\nIf you\\'ve used Beautiful Soup in a project you\\'d like me to know\\nabout, please do send email to me or the discussion\\ngroup.\\n\\nDevelopment\\nDevelopment happens at Launchpad. You can get the source\\ncode or file\\nbugs.\\nThis document is part of Crummy, the webspace of Leonard Richardson (contact information). It was last modified on Sunday, August 24 2025, 14:32:08 Nowhere Standard Time and last built on Friday, August 29 2025, 20:00:02 Nowhere Standard Time.Crummy is © 1996-2025 Leonard Richardson. Unless otherwise noted, all text licensed under a Creative Commons License.Document tree:\\nhttp://www.crummy.com/software/BeautifulSoup/\\n\\n\\n\\n\\nSite Search:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e130883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Download\n",
      "bs4/doc/\n",
      "#HallOfFame\n",
      "enterprise.html\n",
      "https://code.launchpad.net/beautifulsoup\n",
      "https://git.launchpad.net/beautifulsoup/tree/CHANGELOG\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "zine/\n",
      "bs4/download/\n",
      "http://lxml.de/\n",
      "http://code.google.com/p/html5lib/\n",
      "bs4/doc/\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "https://tidelift.com/security\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\n",
      "zine/\n",
      "None\n",
      "bs4/download/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "download/3.x/BeautifulSoup-3.2.2.tar.gz\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website\n",
      "None\n",
      "http://www.nytimes.com/2007/10/25/arts/design/25vide.html\n",
      "https://github.com/BlankerL/DXY-COVID-19-Crawler\n",
      "https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\n",
      "https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\n",
      "http://www.harrowell.org.uk/viktormap.html\n",
      "http://svn.python.org/view/tracker/importer/\n",
      "http://www2.ljworld.com/\n",
      "http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\n",
      "http://esrl.noaa.gov/gsd/fab/\n",
      "http://laps.noaa.gov/topograbber/\n",
      "http://groups.google.com/group/beautifulsoup/\n",
      "https://launchpad.net/beautifulsoup\n",
      "https://code.launchpad.net/beautifulsoup/\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "/self/\n",
      "/self/contact.html\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://www.crummy.com/\n",
      "http://www.crummy.com/software/\n",
      "http://www.crummy.com/software/BeautifulSoup/\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'): \n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b962de",
   "metadata": {},
   "source": [
    "## 2 Interacting with API's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd47e04",
   "metadata": {},
   "source": [
    "### Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd59ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('j.json') as json_file: \n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "for k, v in json_data.items(): \n",
    "    print(k, ': ', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5f361",
   "metadata": {},
   "source": [
    "API\n",
    "\n",
    "* Aplication software interfase\n",
    "* Collection of code: protocol and routines\n",
    "* Allos two software programs to comunicate with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bcc753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c3d00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f8ecf",
   "metadata": {},
   "source": [
    "* Response as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "793b6d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'The Social Network',\n",
       " 'Year': '2010',\n",
       " 'Rated': 'PG-13',\n",
       " 'Released': '01 Oct 2010',\n",
       " 'Runtime': '120 min',\n",
       " 'Genre': 'Biography, Drama',\n",
       " 'Director': 'David Fincher',\n",
       " 'Writer': 'Aaron Sorkin, Ben Mezrich',\n",
       " 'Actors': 'Jesse Eisenberg, Andrew Garfield, Justin Timberlake',\n",
       " 'Plot': 'As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.',\n",
       " 'Language': 'English, French',\n",
       " 'Country': 'United States',\n",
       " 'Awards': 'Won 3 Oscars. 174 wins & 188 nominations total',\n",
       " 'Poster': 'https://m.media-amazon.com/images/M/MV5BMjlkNTE5ZTUtNGEwNy00MGVhLThmZjMtZjU1NDE5Zjk1NDZkXkEyXkFqcGc@._V1_SX300.jpg',\n",
       " 'Ratings': [{'Source': 'Internet Movie Database', 'Value': '7.8/10'},\n",
       "  {'Source': 'Rotten Tomatoes', 'Value': '96%'},\n",
       "  {'Source': 'Metacritic', 'Value': '95/100'}],\n",
       " 'Metascore': '95',\n",
       " 'imdbRating': '7.8',\n",
       " 'imdbVotes': '799,416',\n",
       " 'imdbID': 'tt1285016',\n",
       " 'Type': 'movie',\n",
       " 'DVD': 'N/A',\n",
       " 'BoxOffice': '$96,962,694',\n",
       " 'Production': 'N/A',\n",
       " 'Website': 'N/A',\n",
       " 'Response': 'True'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = r.json()\n",
    "json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc17c43",
   "metadata": {},
   "source": [
    "* Response as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5a790b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin, Ben Mezrich\",\"Actors\":\"Jesse Eisenberg, Andrew Garfield, Justin Timberlake\",\"Plot\":\"As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"United States\",\"Awards\":\"Won 3 Oscars. 174 wins & 188 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BMjlkNTE5ZTUtNGEwNy00MGVhLThmZjMtZjU1NDE5Zjk1NDZkXkEyXkFqcGc@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.8/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.8\",\"imdbVotes\":\"799,416\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"N/A\",\"BoxOffice\":\"$96,962,694\",\"Production\":\"N/A\",\"Website\":\"N/A\",\"Response\":\"True\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = r.text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3022ce2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65688225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
